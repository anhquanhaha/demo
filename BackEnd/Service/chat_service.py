"""
Chat Service - Xá»­ lÃ½ logic chat vÃ  streaming
"""
from typing import List, AsyncGenerator, Dict, Any, Optional
import json
import base64
import asyncio
import time
from Model import ChatMessage
from agent import agent, AgentManager
from fastapi import UploadFile
from database import db_manager


class ChatService:
    """
    Service class Ä‘á»ƒ xá»­ lÃ½ cÃ¡c chá»©c nÄƒng chat
    """
    
    @staticmethod
    async def generate_stream_response(messages: List[ChatMessage]) -> AsyncGenerator[str, None]:
        """
        Generator function Ä‘á»ƒ táº¡o streaming response tá»« agent
        
        Args:
            messages: Danh sÃ¡ch cÃ¡c tin nháº¯n chat
            
        Yields:
            str: Server-Sent Events formatted strings
        """
        try:
            # Convert ChatMessage objects to dict format for agent
            agent_messages = []
            for msg in messages:
                agent_messages.append({
                    "role": msg.role, 
                    "content": msg.content
                })
            
            # Gá»i agent vá»›i streaming
            response = agent.astream({"messages": agent_messages})
            
            # Stream tá»«ng chunk cá»§a response
            async for chunk in response:
                # Langgraph agent tráº£ vá» structure: {'agent': {'messages': [...]}}
                if isinstance(chunk, dict):
                    # Kiá»ƒm tra structure cá»§a langgraph
                    if 'agent' in chunk and isinstance(chunk['agent'], dict):
                        agent_data = chunk['agent']
                        if 'messages' in agent_data:
                            for message in agent_data['messages']:
                                if hasattr(message, 'content') and message.content:
                                    # Split content thÃ nh chunks nhá» Ä‘á»ƒ táº¡o streaming effect
                                    content = message.content
                                    chunk_size = 50  # Chia nhá» content
                                    
                                    for i in range(0, len(content), chunk_size):
                                        chunk_content = content[i:i + chunk_size]
                                        data = {
                                            "type": "chunk",
                                            "content": chunk_content,
                                            "role": "assistant"
                                        }
                                        yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
                                        
                                        # ThÃªm delay nhá» Ä‘á»ƒ táº¡o streaming effect
                                        await asyncio.sleep(0.05)
                    
                    # Fallback cho structure cÅ©
                    elif 'messages' in chunk:
                        for message in chunk['messages']:
                            if hasattr(message, 'content') and message.content:
                                data = {
                                    "type": "message",
                                    "content": message.content,
                                    "role": "assistant"
                                }
                                yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
            
            # Gá»­i signal káº¿t thÃºc stream
            yield f"data: {json.dumps({'type': 'end'}, ensure_ascii=False)}\n\n"
            
        except Exception as e:
            # Gá»­i lá»—i qua stream
            error_data = {
                "type": "error",
                "message": str(e)
            }
            yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
    
    @staticmethod
    async def process_chat_request(messages: List[ChatMessage]) -> Dict[str, Any]:
        """
        Xá»­ lÃ½ chat request khÃ´ng streaming
        
        Args:
            messages: Danh sÃ¡ch cÃ¡c tin nháº¯n chat
            
        Returns:
            Dict: Response data vá»›i role vÃ  content
            
        Raises:
            Exception: Náº¿u cÃ³ lá»—i trong quÃ¡ trÃ¬nh xá»­ lÃ½
        """
        try:
            # Convert ChatMessage objects to dict format for agent
            agent_messages = []
            for msg in messages:
                agent_messages.append({
                    "role": msg.role,
                    "content": msg.content
                })
            
            # Gá»i agent
            result = agent.invoke({"messages": agent_messages})
            
            # Extract response content
            if 'messages' in result and len(result['messages']) > 0:
                last_message = result['messages'][-1]
                return {
                    "role": "assistant",
                    "content": last_message.content if hasattr(last_message, 'content') else str(last_message)
                }
            else:
                return {
                    "role": "assistant", 
                    "content": "Xin lá»—i, tÃ´i khÃ´ng thá»ƒ xá»­ lÃ½ yÃªu cáº§u cá»§a báº¡n."
                }
                
        except Exception as e:
            raise Exception(f"Lá»—i xá»­ lÃ½ chat: {str(e)}")
    
    @staticmethod
    async def _process_file_attachment(file_attachment: Optional[UploadFile]) -> tuple[Optional[str], Optional[str]]:
        """
        Xá»­ lÃ½ file attachment riÃªng biá»‡t Ä‘á»ƒ trÃ¡nh lá»—i I/O
        
        Returns:
            tuple: (file_name, file_content)
        """
        if not file_attachment:
            return None, None
            
        file_name = file_attachment.filename or "unknown_file"
        file_content = None
        
        try:
            # LuÃ´n cá»‘ gáº¯ng reset file pointer vá» Ä‘áº§u file vÃ  Ä‘á»c ngay
            try:
                await file_attachment.seek(0)
            except Exception:
                pass
            
            try:
                file_bytes = await file_attachment.read()
            except Exception as read_error:
                # Thá»­ seek rá»“i Ä‘á»c láº¡i má»™t láº§n
                try:
                    await file_attachment.seek(0)
                    file_bytes = await file_attachment.read()
                except Exception:
                    file_content = f"[ERROR READING FILE: {file_name}] - {str(read_error)}"
                    return file_name, file_content
            
            # Äáº£m báº£o file_bytes khÃ´ng rá»—ng
            if not file_bytes or len(file_bytes) == 0:
                file_content = f"[EMPTY FILE: {file_name}]"
            else:
                # Náº¿u lÃ  file áº£nh
                if file_attachment.content_type and file_attachment.content_type.startswith('image/'):
                    # Táº¡o base64 string tá»« bytes cho future use
                    try:
                        base64_string = base64.b64encode(file_bytes).decode('utf-8')
                        file_content = f"[IMAGE: {file_name}] - Size: {len(file_bytes)} bytes, Content-Type: {file_attachment.content_type}"
                    except Exception as b64_error:
                        file_content = f"[IMAGE PROCESSING ERROR: {file_name}] - {str(b64_error)}"
                else:
                    # Náº¿u lÃ  text file, Ä‘á»c content
                    try:
                        file_content = file_bytes.decode('utf-8')
                        # Giá»›i háº¡n Ä‘á»™ dÃ i content Ä‘á»ƒ trÃ¡nh message quÃ¡ dÃ i
                        if len(file_content) > 2000:
                            file_content = file_content[:2000] + "... (truncated)"
                    except UnicodeDecodeError:
                        # Thá»­ vá»›i cÃ¡c encoding khÃ¡c
                        try:
                            file_content = file_bytes.decode('latin-1')
                            if len(file_content) > 2000:
                                file_content = file_content[:2000] + "... (truncated)"
                        except Exception:
                            file_content = f"[BINARY FILE: {file_name}] - Size: {len(file_bytes)} bytes"
                        
        except Exception as file_error:
            # Log chi tiáº¿t lá»—i Ä‘á»ƒ debug
            error_details = f"Type: {type(file_error).__name__}, Message: {str(file_error)}"
            file_content = f"[CRITICAL ERROR READING FILE: {file_name}] - {error_details}"
            
        finally:
            # KhÃ´ng Ä‘Ã³ng file táº¡i Ä‘Ã¢y Ä‘á»ƒ trÃ¡nh áº£nh hÆ°á»Ÿng tá»›i lifecycle cá»§a framework
            pass
            
        return file_name, file_content

    @staticmethod
    async def process_agent_testcase_stream(
        conversation_id: Optional[str],
        title: str,
        pbi_requirement: str,
        file_attachment: Optional[UploadFile] = None,
        preloaded_file_name: Optional[str] = None,
        preloaded_file_content: Optional[str] = None,
        preloaded_base64_data: Optional[str] = None,
        preloaded_mime_type: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """
        Xá»­ lÃ½ agent testcase request vá»›i streaming response
        
        Args:
            conversation_id: Thread ID cho memory persistence
            title: TiÃªu Ä‘á» cá»§a testcase
            pbi_requirement: YÃªu cáº§u PBI
            file_attachment: File Ä‘Ã­nh kÃ¨m (náº¿u cÃ³)
            
        Yields:
            str: Server-Sent Events formatted strings
        """
        try:
            # Æ¯u tiÃªn dÃ¹ng ná»™i dung file Ä‘Ã£ Ä‘Æ°á»£c preload á»Ÿ endpoint
            if preloaded_file_name is not None or preloaded_file_content is not None:
                file_name, file_content = preloaded_file_name, preloaded_file_content
            else:
                # Náº¿u chÆ°a preload, xá»­ lÃ½ ngay Ä‘áº§u stream
                file_name, file_content = await ChatService._process_file_attachment(file_attachment)
            
            # Táº¡o message vá»›i context Ä‘áº§y Ä‘á»§ (khÃ´ng truyá»n file content)
            message_content = AgentManager.create_message_with_context(
                title=title,
                pbi_requirement=pbi_requirement,
                has_file=file_content is not None,
                file_name=file_name
            )
            
            # LÆ°u user message vÃ o database náº¿u cÃ³ conversation_id
            if conversation_id:
                db_manager.add_message(conversation_id, "user", message_content)
            
            # Táº¡o config cho agent vá»›i thread_id
            # LuÃ´n cáº§n thread_id Ä‘á»ƒ sá»­ dá»¥ng memory checkpointer
            thread_id = conversation_id or f"temp_{int(time.time())}"
            config = {"configurable": {"thread_id": thread_id}}
            
            # Táº¡o message content cho agent (multimodal náº¿u cÃ³ áº£nh)
            agent_message = {"role": "user"}
            
            # Kiá»ƒm tra náº¿u cÃ³ file áº£nh thÃ¬ táº¡o multimodal message
            if (file_content and file_name and 
                (file_content.startswith("[IMAGE:") or 
                 (preloaded_file_content and preloaded_file_content.startswith("[IMAGE:")))):
                
                # Sá»­ dá»¥ng preloaded base64 data vÃ  mime type
                base64_data = preloaded_base64_data
                mime_type = preloaded_mime_type or "image/jpeg"  # default
                
                # Náº¿u cÃ³ base64 data thÃ¬ táº¡o multimodal message
                if base64_data:
                    agent_message["content"] = [
                        {
                            "type": "text",
                            "text": message_content,
                        },
                        {
                            "type": "image",
                            "source_type": "base64",
                            "data": base64_data,
                            "mime_type": mime_type,
                        },
                    ]
                else:
                    # Fallback to text only náº¿u khÃ´ng láº¥y Ä‘Æ°á»£c base64
                    agent_message["content"] = message_content
            else:
                # Message text thÃ´ng thÆ°á»ng náº¿u khÃ´ng cÃ³ áº£nh
                agent_message["content"] = message_content
            
            # Gá»i agent vá»›i streaming vÃ  thread_id
            response = agent.astream(
                {"messages": [agent_message]},
                config=config
            )
            
            # Biáº¿n Ä‘á»ƒ lÆ°u full response content
            full_response_content = ""
            
            # Stream tá»«ng chunk cá»§a response
            async for chunk in response:
                # Langgraph agent tráº£ vá» structure: {'agent': {'messages': [...]}}
                if isinstance(chunk, dict):
                    # Kiá»ƒm tra structure cá»§a langgraph
                    if 'agent' in chunk and isinstance(chunk['agent'], dict):
                        agent_data = chunk['agent']
                        if 'messages' in agent_data:
                            for message in agent_data['messages']:
                                if hasattr(message, 'content') and message.content:
                                    # LÆ°u full content Ä‘á»ƒ save vÃ o database sau
                                    full_response_content = message.content
                                    
                                    # Split content thÃ nh chunks nhá» Ä‘á»ƒ táº¡o streaming effect
                                    content = message.content
                                    chunk_size = 50  # Chia nhá» content
                                    
                                    for i in range(0, len(content), chunk_size):
                                        chunk_content = content[i:i + chunk_size]
                                        data = {
                                            "type": "chunk",
                                            "content": chunk_content,
                                            "role": "assistant",
                                            "conversation_id": conversation_id
                                        }
                                        yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
                                        
                                        # ThÃªm delay nhá» Ä‘á»ƒ táº¡o streaming effect
                                        await asyncio.sleep(0.05)
                    
                    # Fallback cho structure cÅ©
                    elif 'messages' in chunk:
                        for message in chunk['messages']:
                            if hasattr(message, 'content') and message.content:
                                full_response_content = message.content
                                data = {
                                    "type": "message",
                                    "content": message.content,
                                    "role": "assistant",
                                    "conversation_id": conversation_id
                                }
                                yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
            
            # LÆ°u assistant response vÃ o database náº¿u cÃ³ conversation_id
            if conversation_id and full_response_content:
                db_manager.add_message(conversation_id, "assistant", full_response_content)
            
            # Gá»­i signal káº¿t thÃºc stream
            yield f"data: {json.dumps({'type': 'end', 'conversation_id': conversation_id}, ensure_ascii=False)}\n\n"
            
        except Exception as e:
            # Gá»­i lá»—i qua stream
            error_data = {
                "type": "error",
                "message": str(e),
                "conversation_id": conversation_id
            }
            yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
    
    @staticmethod
    async def process_agent_testcase_edit_stream(
        conversation_id: str,
        prompt: str
    ) -> AsyncGenerator[str, None]:
        """
        Xá»­ lÃ½ agent testcase edit request vá»›i streaming response
        
        Args:
            conversation_id: Thread ID cá»§a conversation cáº§n edit
            prompt: YÃªu cáº§u chá»‰nh sá»­a tá»« ngÆ°á»i dÃ¹ng
            
        Yields:
            str: Server-Sent Events formatted strings
        """
        try:
            # Láº¥y lá»‹ch sá»­ conversation tá»« database
            messages = db_manager.get_messages_by_conversation_id(conversation_id)
            
            if not messages:
                error_data = {
                    "type": "error",
                    "message": "KhÃ´ng tÃ¬m tháº¥y lá»‹ch sá»­ conversation",
                    "conversation_id": conversation_id
                }
                yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
                return
            
            # Táº¡o edit message vá»›i context
            edit_message_content = ChatService._create_edit_message_with_context(
                messages=messages,
                edit_prompt=prompt
            )
            
            # LÆ°u user edit message vÃ o database
            db_manager.add_message(conversation_id, "user", f"[EDIT REQUEST] {prompt}")
            
            # Táº¡o config cho agent vá»›i thread_id Ä‘á»ƒ sá»­ dá»¥ng memory
            config = {"configurable": {"thread_id": conversation_id}}
            
            # Gá»i agent vá»›i streaming vÃ  thread_id
            response = agent.astream(
                {"messages": [{"role": "user", "content": edit_message_content}]},
                config=config
            )
            
            # Biáº¿n Ä‘á»ƒ lÆ°u full response content
            full_response_content = ""
            
            # Stream tá»«ng chunk cá»§a response
            async for chunk in response:
                # Langgraph agent tráº£ vá» structure: {'agent': {'messages': [...]}}
                if isinstance(chunk, dict):
                    # Kiá»ƒm tra structure cá»§a langgraph
                    if 'agent' in chunk and isinstance(chunk['agent'], dict):
                        agent_data = chunk['agent']
                        if 'messages' in agent_data:
                            for message in agent_data['messages']:
                                if hasattr(message, 'content') and message.content:
                                    # LÆ°u full content Ä‘á»ƒ save vÃ o database sau
                                    full_response_content = message.content
                                    
                                    # Split content thÃ nh chunks nhá» Ä‘á»ƒ táº¡o streaming effect
                                    content = message.content
                                    chunk_size = 50  # Chia nhá» content
                                    
                                    for i in range(0, len(content), chunk_size):
                                        chunk_content = content[i:i + chunk_size]
                                        data = {
                                            "type": "chunk",
                                            "content": chunk_content,
                                            "role": "assistant",
                                            "conversation_id": conversation_id
                                        }
                                        yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
                                        
                                        # ThÃªm delay nhá» Ä‘á»ƒ táº¡o streaming effect
                                        await asyncio.sleep(0.05)
                    
                    # Fallback cho structure cÅ©
                    elif 'messages' in chunk:
                        for message in chunk['messages']:
                            if hasattr(message, 'content') and message.content:
                                full_response_content = message.content
                                data = {
                                    "type": "message",
                                    "content": message.content,
                                    "role": "assistant",
                                    "conversation_id": conversation_id
                                }
                                yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
            
            # LÆ°u assistant response vÃ o database
            if full_response_content:
                db_manager.add_message(conversation_id, "assistant", full_response_content)
            
            # Gá»­i signal káº¿t thÃºc stream
            yield f"data: {json.dumps({'type': 'end', 'conversation_id': conversation_id}, ensure_ascii=False)}\n\n"
            
        except Exception as e:
            # Gá»­i lá»—i qua stream
            error_data = {
                "type": "error",
                "message": str(e),
                "conversation_id": conversation_id
            }
            yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
    
    @staticmethod
    def _create_edit_message_with_context(messages: List[Dict[str, Any]], edit_prompt: str) -> str:
        """
        Táº¡o message vá»›i context cho edit request
        
        Args:
            messages: Lá»‹ch sá»­ messages tá»« database
            edit_prompt: YÃªu cáº§u chá»‰nh sá»­a tá»« ngÆ°á»i dÃ¹ng
            
        Returns:
            str: Message Ä‘Ã£ Ä‘Æ°á»£c format vá»›i context
        """
        # Táº¡o context tá»« lá»‹ch sá»­ conversation
        context_parts = ["**Lá»ŠCH Sá»¬ CONVERSATION:**\n"]
        
        for msg in messages:
            role_label = "ğŸ‘¤ NGÆ¯á»œI DÃ™NG" if msg['role'] == 'user' else "ğŸ¤– ASSISTANT"
            context_parts.append(f"{role_label}: {msg['content']}\n")
        
        # ThÃªm yÃªu cáº§u edit
        context_parts.extend([
            "\n" + "="*50,
            "\n**YÃŠU Cáº¦U CHá»ˆNH Sá»¬A Má»šI:**",
            f"\n{edit_prompt}",
            "\n" + "="*50,
            "\nHÃ£y dá»±a vÃ o lá»‹ch sá»­ conversation á»Ÿ trÃªn vÃ  yÃªu cáº§u chá»‰nh sá»­a má»›i Ä‘á»ƒ:",
            "1. Hiá»ƒu rÃµ context vÃ  ná»™i dung Ä‘Ã£ tháº£o luáº­n trÆ°á»›c Ä‘Ã³",
            "2. Thá»±c hiá»‡n chá»‰nh sá»­a theo yÃªu cáº§u má»›i",
            "3. ÄÆ°a ra káº¿t quáº£ Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t/sá»­a Ä‘á»•i",
            "4. Giáº£i thÃ­ch nhá»¯ng thay Ä‘á»•i Ä‘Ã£ thá»±c hiá»‡n (náº¿u cáº§n)",
            "\nVui lÃ²ng tráº£ lá»i báº±ng tiáº¿ng Viá»‡t."
        ])
        
        return "\n".join(context_parts)
